{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Convolutional Neural Network from Scratch\n",
    "\n",
    "[**1. Convolution functions**](#1.-Convolution-functions)\n",
    "- Zero Padding\n",
    "- Convolve window\n",
    "- Convolution forward\n",
    "- Convolution backward\n",
    "\n",
    "[**2. Pooling functions**](#2.-Pooling-functions)\n",
    "- Pooling forward\n",
    "- Create mask (max pooling - backward pass)\n",
    "- Distribute value (average pooling - backward pass)\n",
    "- Pooling backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolution functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1. Zero-Padding\n",
    "\n",
    "Adding zeros around the border of all images of the dataset X.\n",
    "\n",
    "**Inputs** :  \n",
    "- *X*: python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "- *pad*: integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "**Outputs** :  \n",
    "- *X_pad*: padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "\n",
    "    X_pad = np.pad(X, ((0,0),(pad,pad),(pad,pad),(0,0)), mode='constant', constant_values = (0,0))\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 7, 7, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19a57be9af0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2. Convolution window\n",
    "\n",
    "Applying one filter defined by parameters W on a single slice (a_slice_prev) of the output activation of the previous layer.\n",
    "\n",
    "**Inputs** :  \n",
    "- *a_slice_prev*: slice of input data of shape (f, f, n_C_prev)\n",
    "- *W*: weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "- *b*: bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "**Outputs** :  \n",
    "- *Z*: a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "\n",
    "    s = a_slice_prev * W\n",
    "    Z = np.sum(s)\n",
    "    Z = Z + float(b)\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3. Convolution forward\n",
    "\n",
    "Implementing the forward propagation for a convolution function.\n",
    "\n",
    "**Inputs** :  \n",
    "- *A_prev*: output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- *W*: weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "- *b*: biases, numpy array of shape (1, 1, 1, n_C)\n",
    "- *hparameters*: python dictionary containing \"stride\" and \"pad\"\n",
    "\n",
    "**Outputs** :  \n",
    "- *Z*: conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "- *cache*: cache of values needed for the conv_backward() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    n_H = int((n_H_prev - f + (2*pad))/stride) + 1\n",
    "    n_W = int((n_W_prev - f + (2*pad))/stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        for h in range(n_H):\n",
    "            vert_start = h*stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range(n_C):\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    weights = W[:, :, :, c]\n",
    "                    biases = b[0, 0, 0, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.6923608807576933\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4)\n",
    "W = np.random.randn(3,3,4,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 1, \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\\n\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\\n\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4. Convolution Backward\n",
    "\n",
    "Implementing the backward propagation for a convolution function.\n",
    "\n",
    "**Inputs** :  \n",
    "- *dZ*: gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "- *cache*: cache of values needed for the conv_backward(), output of conv_forward()\n",
    "\n",
    "**Outputs** :  \n",
    "- *dA_prev*: gradient of the cost with respect to the input of the conv layer (A_prev), numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- *dW*: gradient of the cost with respect to the weights of the conv layer (W), numpy array of shape (f, f, n_C_prev, n_C)\n",
    "- *db*: gradient of the cost with respect to the biases of the conv layer (b), numpy array of shape (1, 1, 1, n_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2, \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pooling functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1. Pooling forward\n",
    "\n",
    "Implementing the forward pass of the pooling layer.\n",
    "\n",
    "**Inputs** :  \n",
    "- *A_prev*: output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "- *hparameters*: python dictionary containing \"f\" and \"stride\"\n",
    "- *mode*: the pooling mode we would like to use, defined as a string (\"max\" or \"average\")\n",
    "\n",
    "**Outputs** :  \n",
    "- *A*: output of the pool layer, numpy array of shape (m, n_H, n_W, n_C)\n",
    "- *cache*: cache used in the backward pass of the pooling layer, contains the input and hparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "\n",
    "    for i in range(m):\n",
    "        for h in range(n_H):\n",
    "            vert_start = h*stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = horiz_start + f\n",
    "                for c in range (n_C):\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    cache = (A_prev, hparameters)\n",
    "\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2. Create mask (max pooling - backward pass)\n",
    "\n",
    "Creating a mask from an input matrix x, to identify the max entry of x.\n",
    "\n",
    "**Inputs** :  \n",
    "- *x*: array of shape (f, f)\n",
    "\n",
    "**Outputs** :  \n",
    "- *mask*: array of the same shape as window, contains a True at the position corresponding to the max entry of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "\n",
    "    mask = (x == np.max(x))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.3. Distribute value (average pooling - backward pass)\n",
    "\n",
    "Distributing the input value in the matrix of dimension shape.\n",
    "\n",
    "**Inputs** :  \n",
    "- *dz*: input scalar\n",
    "- *shape*: the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "\n",
    "**Outputs** :  \n",
    "- *a*: array of size (n_H, n_W) for which we distributed the value of dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    average = dz / (n_H * n_W)\n",
    "    a = np.ones((n_H, n_W)) * average\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4. Pooling backward\n",
    "\n",
    "Implementing the backward pass of the pooling layer.\n",
    "\n",
    "**Inputs** :  \n",
    "- *dA*: gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "- *cache*: cache output from the forward pass of the pooling layer, contains the layer's input and hparameters\n",
    "- *mode*: the pooling mode we would like to use, defined as a string (\"max\" or \"average\")\n",
    "\n",
    "**Outputs** :  \n",
    "- *dA_prev*: gradient of cost with respect to the input of the pooling layer, same shape as A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "\n",
    "    (A_prev, hparameters) = cache\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    if mode == \"max\":\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += (mask * dA[i, h, w, c])\n",
    "                    elif mode == \"average\":\n",
    "                        da = dA[i, h, w, c]\n",
    "                        shape = (f,f)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
