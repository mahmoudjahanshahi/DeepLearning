{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**1. Initialization**](#1.-Initialization)\n",
    "\n",
    "[**2. Model Building**](#2.-Model-Building)\n",
    "\n",
    "[**3. Hyperparameter Tunning**](#3.-Hyperparameter-Tunning)\n",
    "\n",
    "[**4. Predicting and Evaluation**](#4.-Predicting-and-Evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**m**: total number of samples  \n",
    "**n_x**: number of features  \n",
    "**X**: input data  \n",
    "**Y**: labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (11, 1599)\n",
      "The shape of Y is: (1, 1599)\n",
      "Total number of examples: 1599\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"winequality-red.csv\")\n",
    "\n",
    "shape_data = data.shape\n",
    "m = shape_data[0]\n",
    "n_x = shape_data[1]\n",
    "\n",
    "X = np.reshape(np.array(data.iloc[:,:n_x-1]),(m,n_x-1))\n",
    "X = X.T\n",
    "Y = np.reshape(np.array(data.iloc[:,n_x-1]),(m,1))\n",
    "Y = Y.T\n",
    "\n",
    "shape_X = np.shape(X)\n",
    "shape_Y = np.shape(Y)\n",
    "\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('Total number of examples: %d' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Deviding Data into Train, Cross Validation and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train**: training set  \n",
    "**cv**: cross validation set  \n",
    "**test**: test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = int(m*0.6)\n",
    "m_cv = int(m*0.2)\n",
    "m_test = m - m_train - m_cv\n",
    "\n",
    "X_train , X_cv , X_test = np.split(X , [m_train,m_train+m_cv] , axis=1)\n",
    "Y_train , Y_cv , Y_test = np.split(Y , [m_train,m_train+m_cv] , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Defining neural network structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_x**: the size of the input layer (this is number of features)  \n",
    "**n_h**: the size of the hidden layer (this is initially set to 4, but will be tunned in next section)  \n",
    "**n_y**: the size of the output layer (based on the data, Y has 6 unique values from 3 to 8 inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x\n",
    "n_h = 4\n",
    "n_y = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Initializing the model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parameters**: python dictionary containing:  \n",
    "*W1*: weight matrix of shape (n_h, n_x)  \n",
    "*b1*: bias vector of shape (n_h, 1)  \n",
    "*W2*: weight matrix of shape (n_y, n_h)  \n",
    "*b2*: bias vector of shape (n_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Defining Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Implementing forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z1**: W1\\*X+b1  \n",
    "**A1**: The tanh output of the first layer activation  \n",
    "**Z2**: W2\\*A1+b2  \n",
    "**A2**: The sigmoid output of the second layer activation  \n",
    "**tmp**: a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1)+b2\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Computing Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cost**: cross-entropy cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A2)+(1-Y)*np.log(1-A2))\n",
    "    cost = float(np.squeeze(cost))  # making sure cost is the dimension we expect!\n",
    "                                    \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Implementing backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**grads**: python dictionary containing gradients with respect to different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, tmp, X, Y):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "   \n",
    "    A1 = tmp[\"A1\"]\n",
    "    A2 = tmp[\"A2\"]\n",
    "\n",
    "    dZ2 = A2-Y\n",
    "    dW2 = (1/m)*np.dot(dZ2,A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Parameters update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Integrating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, tmp = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    "        grads = backward_propagation(parameters, tmp, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
